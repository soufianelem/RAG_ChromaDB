{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3699d895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c315fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e516e2",
   "metadata": {},
   "source": [
    "## Beispieldaten erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73d2e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files created in 'data' folder: ['rag_intro.txt', 'rag_architecture.txt', 'rag_usecases.txt', 'rag_challenges.txt', 'rag_future.txt', 'rag_vs_finetuningvs_promptengineering.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "sample_docs = {\n",
    "    \"rag_intro.txt\": \"\"\"Retrieval-Augmented Generation (RAG) is a technique \n",
    "that combines information retrieval with generative models. \n",
    "It allows language models to access external knowledge sources \n",
    "to provide more accurate and up-to-date answers.\"\"\",\n",
    "\n",
    "    \"rag_architecture.txt\": \"\"\"The RAG architecture typically consists of two parts: \n",
    "a retriever and a generator. The retriever finds relevant documents \n",
    "from a knowledge base, while the generator (often a large language model) \n",
    "uses those documents to produce responses.\"\"\",\n",
    "\n",
    "    \"rag_usecases.txt\": \"\"\"RAG can be applied in many areas such as \n",
    "question answering, chatbots, document summarization, and enterprise search. \n",
    "It improves reliability by grounding responses in factual documents.\"\"\",\n",
    "\n",
    "\"rag_challenges.txt\": \"\"\"Some challenges of RAG include retrieving irrelevant documents,\n",
    "managing large-scale knowledge bases, and ensuring that the generator\n",
    "uses the retrieved documents correctly without introducing hallucinations.\"\"\",\n",
    "\n",
    "    \"rag_future.txt\": \"\"\"The future of RAG research focuses on improving retriever models,\n",
    "combining structured and unstructured data, and optimizing efficiency\n",
    "for real-world enterprise applications.\"\"\",\n",
    "\n",
    "    \"rag_vs_finetuningvs_promptengineering.txt\": \"\"\"RAG, fine-tuning, and prompt engineering differ in how they improve \n",
    "language models.\n",
    "\n",
    "Fine-tuning changes the model’s weights by training it on additional \n",
    "domain-specific data. This makes the model more accurate for that domain, \n",
    "but it is costly and needs retraining when knowledge changes.\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) does not change the model itself. \n",
    "Instead, it connects the model to an external retriever that provides \n",
    "relevant documents. The model then generates answers based on both its \n",
    "own knowledge and the retrieved context. This keeps results more current \n",
    "and fact-based.\n",
    "\n",
    "Prompt engineering does not retrain the model or add external data. \n",
    "It only improves how we ask questions by designing effective prompts. \n",
    "This method is simple and fast but limited, since it does not add \n",
    "new knowledge.\n",
    "\n",
    "In short: fine-tuning embeds knowledge, RAG retrieves knowledge, \n",
    "and prompt engineering shapes the way knowledge is used.\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filename, content in sample_docs.items():\n",
    "    with open(os.path.join(\"data\", filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Text files created in 'data' folder:\", list(sample_docs.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02003b5",
   "metadata": {},
   "source": [
    "## Text in Chunks aufteilen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66faeccd",
   "metadata": {},
   "source": [
    "## Dokumente laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0fe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents\n",
      "\n",
      "First document preview:\n",
      "The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "u...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"data\",                     \n",
    "    glob=\"*.txt\",              \n",
    "    loader_cls=TextLoader,      \n",
    "    loader_kwargs={'encoding': 'utf-8'}  \n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"{len(documents)} Dokumente geladen\")\n",
    "print(\"\\nVorschau erstes Dokument:\")\n",
    "print(documents[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed490a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8 chunks from 6 documents\n",
      "\n",
      "Chunk example:\n",
      "Content: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, whi...\n",
      "Metadata: {'source': 'data\\\\rag_architecture.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,     \n",
    "    chunk_overlap=50,   \n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(chunks)} Chunks aus {len(documents)} Dokumenten erstellt\")\n",
    "\n",
    "print(\"\\nChunk-Beispiel:\")\n",
    "print(f\"Inhalt: {chunks[0].page_content[:150]}...\")\n",
    "print(f\"Metadaten: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0869b663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\rag_architecture.txt'}, page_content='The RAG architecture typically consists of two parts: \\na retriever and a generator. The retriever finds relevant documents \\nfrom a knowledge base, while the generator (often a large language model) \\nuses those documents to produce responses.'),\n",
       " Document(metadata={'source': 'data\\\\rag_challenges.txt'}, page_content='Some challenges of RAG include retrieving irrelevant documents,\\nmanaging large-scale knowledge bases, and ensuring that the generator\\nuses the retrieved documents correctly without introducing hallucinations.'),\n",
       " Document(metadata={'source': 'data\\\\rag_future.txt'}, page_content='The future of RAG research focuses on improving retriever models,\\ncombining structured and unstructured data, and optimizing efficiency\\nfor real-world enterprise applications.'),\n",
       " Document(metadata={'source': 'data\\\\rag_intro.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a technique \\nthat combines information retrieval with generative models. \\nIt allows language models to access external knowledge sources \\nto provide more accurate and up-to-date answers.'),\n",
       " Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG (Retrieval-Augmented Generation) does not change the model itself. \\nInstead, it connects the model to an external retriever that provides \\nrelevant documents. The model then generates answers based on both its \\nown knowledge and the retrieved context. This keeps results more current \\nand fact-based.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='Prompt engineering does not retrain the model or add external data. \\nIt only improves how we ask questions by designing effective prompts. \\nThis method is simple and fast but limited, since it does not add \\nnew knowledge.\\n\\nIn short: fine-tuning embeds knowledge, RAG retrieves knowledge, \\nand prompt engineering shapes the way knowledge is used.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953e64a",
   "metadata": {},
   "source": [
    "## Embedding-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f502daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac263f",
   "metadata": {},
   "source": [
    "## ChromaDB Vektordatenbank erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 16 vectors\n",
      "Persisted to: ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "persist_directory = \"./chroma_db\"   # Ordner für persistente Speicherung\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,                     # Die aufgeteilten Textdokumente\n",
    "    embedding=OpenAIEmbeddings(),         # Embedding-Modell (Text → Vektoren)\n",
    "    persist_directory=persist_directory,  # Datenbank auf Festplatte speichern\n",
    "    collection_name=\"rag_collection\"      # Name der Sammlung in Chroma\n",
    ")\n",
    "\n",
    "print(f\"Vektordatenbank erstellt mit {vectorstore._collection.count()} Vektoren\")\n",
    "print(f\"Gespeichert in: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b770b1c",
   "metadata": {},
   "source": [
    "## Ähnlichkeitssuche testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bfc6cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.'),\n",
       " Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the difference between RAG and fine-tuning?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Content: RAG, fine-tuning, and prompt engineering differ in how they improve \n",
      "language models.\n",
      "\n",
      "Fine-tuning changes the model’s weights by training it on additional \n",
      "domain-specific data. This makes the model more accurate for that domain, \n",
      "but it is costly and needs retraining when knowledge changes.\n",
      "Metadata: {'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}\n",
      "\n",
      "Result 2:\n",
      "Content: RAG, fine-tuning, and prompt engineering differ in how they improve \n",
      "language models.\n",
      "\n",
      "Fine-tuning changes the model’s weights by training it on additional \n",
      "domain-specific data. This makes the model more accurate for that domain, \n",
      "but it is costly and needs retraining when knowledge changes.\n",
      "Metadata: {'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}\n",
      "\n",
      "Result 3:\n",
      "Content: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadata: {'source': 'data\\\\rag_usecases.txt'}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "   print(f\"\\nErgebnis {i}:\")\n",
    "   print(f\"Inhalt: {res.page_content}\")\n",
    "   print(f\"Metadaten: {res.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c951683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "Content: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadata: {'source': 'data\\\\rag_usecases.txt'}\n",
      "\n",
      "Result 2:\n",
      "Content: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadata: {'source': 'data\\\\rag_usecases.txt'}\n",
      "\n",
      "Result 3:\n",
      "Content: Some challenges of RAG include retrieving irrelevant documents,\n",
      "managing large-scale knowledge bases, and ensuring that the generator\n",
      "uses the retrieved documents correctly without introducing hallucinations.\n",
      "Metadata: {'source': 'data\\\\rag_challenges.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are some use cases of RAG?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"\\nErgebnis {i}:\")\n",
    "    print(f\"Inhalt: {res.page_content}\")\n",
    "    print(f\"Metadaten: {res.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51672581",
   "metadata": {},
   "source": [
    "## Ähnlichkeitssuche mit Bewertung\n",
    "\n",
    "ChromaDB verwendet standardmäßig L2-Distanz:\n",
    "- 0.0 = perfekte Übereinstimmung  \n",
    "- Näher zu 0 = relevanter\n",
    "- Höhere Werte = weniger relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c5d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1 (score=0.2337):\n",
      "Content: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "uses those documents to produce responses.\n",
      "Metadata: {'source': 'data\\\\rag_architecture.txt'}\n",
      "\n",
      "Result 2 (score=0.2337):\n",
      "Content: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "uses those documents to produce responses.\n",
      "Metadata: {'source': 'data\\\\rag_architecture.txt'}\n",
      "\n",
      "Result 3 (score=0.3461):\n",
      "Content: Some challenges of RAG include retrieving irrelevant documents,\n",
      "managing large-scale knowledge bases, and ensuring that the generator\n",
      "uses the retrieved documents correctly without introducing hallucinations.\n",
      "Metadata: {'source': 'data\\\\rag_challenges.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main components of the RAG architecture?\"\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nErgebnis {i} (Bewertung={score:.4f}):\")\n",
    "    print(f\"Inhalt: {doc.page_content}\")\n",
    "    print(f\"Metadaten: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6abef",
   "metadata": {},
   "source": [
    "## Sprachmodell initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd0cbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "365005e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Small language models are a type of artificial intelligence model that use a smaller amount of data and computational resources compared to larger language models. They are typically used for tasks such as text classification, sentiment analysis, and language generation. Small language models may have limited capabilities compared to larger models, but they are often more efficient and easier to deploy in production settings.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 12, 'total_tokens': 82, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CFhge0YBHDJBTRXy8G3LDftNegRbk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a2b32356-74bd-41fc-abbd-69a9b6524df4-0', usage_metadata={'input_tokens': 12, 'output_tokens': 70, 'total_tokens': 82, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_response=llm.invoke(\"What is Small Language Models\")\n",
    "test_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d64fd89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002108B4EBA60>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002108B4F3580>, root_client=<openai.OpenAI object at 0x000002108B4EBC40>, root_async_client=<openai.AsyncOpenAI object at 0x000002108B4EBF40>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models.base import init_chat_model\n",
    "\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\")\n",
    "#llm=init_chat_model(\"groq:\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4c69b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines or computer systems. These processes include learning, reasoning, problem solving, perception, and language understanding. AI technology is used in a wide range of applications, such as virtual assistants, self-driving cars, medical diagnosis, and natural language processing. AI systems can improve over time through the use of algorithms and data, making them increasingly sophisticated and able to perform complex tasks.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 10, 'total_tokens': 99, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CFhgfq4CMqORmkaBPJ3nCTlq3IRnv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--7c3fdca3-5688-49dc-83f2-d9109e925ef4-0', usage_metadata={'input_tokens': 10, 'output_tokens': 89, 'total_tokens': 99, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7b210",
   "metadata": {},
   "source": [
    "## RAG-Kette erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6002655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d21d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002108B4F3220>, search_kwargs={})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(\n",
    "    search_kwarg={\"k\":3} \n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b71ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant for question-answering tasks.  \n",
    "Use the retrieved context below to answer the user’s question.  \n",
    "\n",
    "- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \n",
    "- Be concise (max. 3 sentences).  \n",
    "- Ground your answer in the context, don’t invent facts.  \n",
    "\n",
    "Context:\n",
    "{context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32622905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002108B4EBA60>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002108B4F3580>, root_client=<openai.OpenAI object at 0x000002108B4EBC40>, root_async_client=<openai.AsyncOpenAI object at 0x000002108B4EBF40>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b263c5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002108B4F3220>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002108B4EBA60>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002108B4F3580>, root_client=<openai.OpenAI object at 0x000002108B4EBC40>, root_async_client=<openai.AsyncOpenAI object at 0x000002108B4EBF40>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "rag_chain=create_retrieval_chain(retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35d0daec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How can I apply RAG in building a chatbot?',\n",
       " 'context': [Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_architecture.txt'}, page_content='The RAG architecture typically consists of two parts: \\na retriever and a generator. The retriever finds relevant documents \\nfrom a knowledge base, while the generator (often a large language model) \\nuses those documents to produce responses.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_architecture.txt'}, page_content='The RAG architecture typically consists of two parts: \\na retriever and a generator. The retriever finds relevant documents \\nfrom a knowledge base, while the generator (often a large language model) \\nuses those documents to produce responses.')],\n",
       " 'answer': 'You can apply RAG in building a chatbot by using the retriever component to find relevant documents from a knowledge base, and the generator component (often a large language model) to produce responses based on those documents. This approach improves reliability by grounding the chatbot responses in factual information, making the interactions more accurate and informative.'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=rag_chain.invoke({\"input\":\"How can I apply RAG in building a chatbot?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "82bfb67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can apply RAG in building a chatbot by using the retriever component to find relevant documents from a knowledge base, and the generator component (often a large language model) to produce responses based on those documents. This approach improves reliability by grounding the chatbot responses in factual information, making the interactions more accurate and informative.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1_ChromaDB (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
