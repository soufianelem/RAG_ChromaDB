{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3699d895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c315fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e516e2",
   "metadata": {},
   "source": [
    "## Beispieldaten erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "73d2e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files created in 'data' folder: ['rag_intro.txt', 'rag_architecture.txt', 'rag_usecases.txt', 'rag_challenges.txt', 'rag_future.txt', 'rag_vs_finetuningvs_promptengineering.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "sample_docs = {\n",
    "    \"rag_intro.txt\": \"\"\"Retrieval-Augmented Generation (RAG) is a technique \n",
    "that combines information retrieval with generative models. \n",
    "It allows language models to access external knowledge sources \n",
    "to provide more accurate and up-to-date answers.\"\"\",\n",
    "\n",
    "    \"rag_architecture.txt\": \"\"\"The RAG architecture typically consists of two parts: \n",
    "a retriever and a generator. The retriever finds relevant documents \n",
    "from a knowledge base, while the generator (often a large language model) \n",
    "uses those documents to produce responses.\"\"\",\n",
    "\n",
    "    \"rag_usecases.txt\": \"\"\"RAG can be applied in many areas such as \n",
    "question answering, chatbots, document summarization, and enterprise search. \n",
    "It improves reliability by grounding responses in factual documents.\"\"\",\n",
    "\n",
    "\"rag_challenges.txt\": \"\"\"Some challenges of RAG include retrieving irrelevant documents,\n",
    "managing large-scale knowledge bases, and ensuring that the generator\n",
    "uses the retrieved documents correctly without introducing hallucinations.\"\"\",\n",
    "\n",
    "    \"rag_future.txt\": \"\"\"The future of RAG research focuses on improving retriever models,\n",
    "combining structured and unstructured data, and optimizing efficiency\n",
    "for real-world enterprise applications.\"\"\",\n",
    "\n",
    "    \"rag_vs_finetuningvs_promptengineering.txt\": \"\"\"RAG, fine-tuning, and prompt engineering differ in how they improve \n",
    "language models.\n",
    "\n",
    "Fine-tuning changes the model’s weights by training it on additional \n",
    "domain-specific data. This makes the model more accurate for that domain, \n",
    "but it is costly and needs retraining when knowledge changes.\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) does not change the model itself. \n",
    "Instead, it connects the model to an external retriever that provides \n",
    "relevant documents. The model then generates answers based on both its \n",
    "own knowledge and the retrieved context. This keeps results more current \n",
    "and fact-based.\n",
    "\n",
    "Prompt engineering does not retrain the model or add external data. \n",
    "It only improves how we ask questions by designing effective prompts. \n",
    "This method is simple and fast but limited, since it does not add \n",
    "new knowledge.\n",
    "\n",
    "In short: fine-tuning embeds knowledge, RAG retrieves knowledge, \n",
    "and prompt engineering shapes the way knowledge is used.\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filename, content in sample_docs.items():\n",
    "    with open(os.path.join(\"data\", filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Text files created in 'data' folder:\", list(sample_docs.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02003b5",
   "metadata": {},
   "source": [
    "## Text in Chunks aufteilen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66faeccd",
   "metadata": {},
   "source": [
    "## Dokumente laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8fe0fe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Dokumente geladen\n",
      "\n",
      "Vorschau erstes Dokument:\n",
      "The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "u...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"data\",                     \n",
    "    glob=\"*.txt\",              \n",
    "    loader_cls=TextLoader,      \n",
    "    loader_kwargs={'encoding': 'utf-8'}  \n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"{len(documents)} Dokumente geladen\")\n",
    "print(\"\\nVorschau erstes Dokument:\")\n",
    "print(documents[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ed490a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Chunks aus 6 Dokumenten erstellt\n",
      "\n",
      "Chunk-Beispiel:\n",
      "Inhalt: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, whi...\n",
      "Metadaten: {'source': 'data\\\\rag_architecture.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,     \n",
    "    chunk_overlap=50,   \n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(chunks)} Chunks aus {len(documents)} Dokumenten erstellt\")\n",
    "\n",
    "print(\"\\nChunk-Beispiel:\")\n",
    "print(f\"Inhalt: {chunks[0].page_content[:150]}...\")\n",
    "print(f\"Metadaten: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0869b663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\rag_architecture.txt'}, page_content='The RAG architecture typically consists of two parts: \\na retriever and a generator. The retriever finds relevant documents \\nfrom a knowledge base, while the generator (often a large language model) \\nuses those documents to produce responses.'),\n",
       " Document(metadata={'source': 'data\\\\rag_challenges.txt'}, page_content='Some challenges of RAG include retrieving irrelevant documents,\\nmanaging large-scale knowledge bases, and ensuring that the generator\\nuses the retrieved documents correctly without introducing hallucinations.'),\n",
       " Document(metadata={'source': 'data\\\\rag_future.txt'}, page_content='The future of RAG research focuses on improving retriever models,\\ncombining structured and unstructured data, and optimizing efficiency\\nfor real-world enterprise applications.'),\n",
       " Document(metadata={'source': 'data\\\\rag_intro.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a technique \\nthat combines information retrieval with generative models. \\nIt allows language models to access external knowledge sources \\nto provide more accurate and up-to-date answers.'),\n",
       " Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG (Retrieval-Augmented Generation) does not change the model itself. \\nInstead, it connects the model to an external retriever that provides \\nrelevant documents. The model then generates answers based on both its \\nown knowledge and the retrieved context. This keeps results more current \\nand fact-based.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='Prompt engineering does not retrain the model or add external data. \\nIt only improves how we ask questions by designing effective prompts. \\nThis method is simple and fast but limited, since it does not add \\nnew knowledge.\\n\\nIn short: fine-tuning embeds knowledge, RAG retrieves knowledge, \\nand prompt engineering shapes the way knowledge is used.')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953e64a",
   "metadata": {},
   "source": [
    "## Embedding-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f502daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac263f",
   "metadata": {},
   "source": [
    "## ChromaDB Vektordatenbank erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fbb4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vektordatenbank erstellt mit 24 Vektoren\n",
      "Gespeichert in: ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "persist_directory = \"./chroma_db\"   # Ordner für persistente Speicherung\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,                     # Die aufgeteilten Textdokumente\n",
    "    embedding=OpenAIEmbeddings(),         # Embedding-Modell (Text → Vektoren)\n",
    "    persist_directory=persist_directory,  # Datenbank auf Festplatte speichern\n",
    "    collection_name=\"rag_collection\"      # Name der Sammlung in Chroma\n",
    ")\n",
    "\n",
    "print(f\"Vektordatenbank erstellt mit {vectorstore._collection.count()} Vektoren\")\n",
    "print(f\"Gespeichert in: {persist_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b770b1c",
   "metadata": {},
   "source": [
    "## Ähnlichkeitssuche testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bfc6cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the difference between RAG and fine-tuning?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0671774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ergebnis 1:\n",
      "Inhalt: RAG, fine-tuning, and prompt engineering differ in how they improve \n",
      "language models.\n",
      "\n",
      "Fine-tuning changes the model’s weights by training it on additional \n",
      "domain-specific data. This makes the model more accurate for that domain, \n",
      "but it is costly and needs retraining when knowledge changes.\n",
      "Metadaten: {'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}\n",
      "\n",
      "Ergebnis 2:\n",
      "Inhalt: RAG, fine-tuning, and prompt engineering differ in how they improve \n",
      "language models.\n",
      "\n",
      "Fine-tuning changes the model’s weights by training it on additional \n",
      "domain-specific data. This makes the model more accurate for that domain, \n",
      "but it is costly and needs retraining when knowledge changes.\n",
      "Metadaten: {'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}\n",
      "\n",
      "Ergebnis 3:\n",
      "Inhalt: RAG, fine-tuning, and prompt engineering differ in how they improve \n",
      "language models.\n",
      "\n",
      "Fine-tuning changes the model’s weights by training it on additional \n",
      "domain-specific data. This makes the model more accurate for that domain, \n",
      "but it is costly and needs retraining when knowledge changes.\n",
      "Metadaten: {'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "   print(f\"\\nErgebnis {i}:\")\n",
    "   print(f\"Inhalt: {res.page_content}\")\n",
    "   print(f\"Metadaten: {res.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2c951683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ergebnis 1:\n",
      "Inhalt: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadaten: {'source': 'data\\\\rag_usecases.txt'}\n",
      "\n",
      "Ergebnis 2:\n",
      "Inhalt: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadaten: {'source': 'data\\\\rag_usecases.txt'}\n",
      "\n",
      "Ergebnis 3:\n",
      "Inhalt: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadaten: {'source': 'data\\\\rag_usecases.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are some use cases of RAG?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"\\nErgebnis {i}:\")\n",
    "    print(f\"Inhalt: {res.page_content}\")\n",
    "    print(f\"Metadaten: {res.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51672581",
   "metadata": {},
   "source": [
    "## Ähnlichkeitssuche mit Bewertung\n",
    "\n",
    "ChromaDB verwendet standardmäßig L2-Distanz:\n",
    "- 0.0 = perfekte Übereinstimmung  \n",
    "- Näher zu 0 = relevanter\n",
    "- Höhere Werte = weniger relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "494c5d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ergebnis 1 (Bewertung=0.2337):\n",
      "Inhalt: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "uses those documents to produce responses.\n",
      "Metadaten: {'source': 'data\\\\rag_architecture.txt'}\n",
      "\n",
      "Ergebnis 2 (Bewertung=0.2337):\n",
      "Inhalt: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "uses those documents to produce responses.\n",
      "Metadaten: {'source': 'data\\\\rag_architecture.txt'}\n",
      "\n",
      "Ergebnis 3 (Bewertung=0.2337):\n",
      "Inhalt: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "uses those documents to produce responses.\n",
      "Metadaten: {'source': 'data\\\\rag_architecture.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main components of the RAG architecture?\"\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nErgebnis {i} (Bewertung={score:.4f}):\")\n",
    "    print(f\"Inhalt: {doc.page_content}\")\n",
    "    print(f\"Metadaten: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6abef",
   "metadata": {},
   "source": [
    "## Sprachmodell initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dd0cbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "365005e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Small language models refer to neural network-based models that are trained on a relatively small amount of text data compared to larger language models such as GPT-3. These models typically have fewer parameters and are trained on a smaller scale in terms of data and computational resources.\\n\\nDespite their smaller size, small language models can still be effective for various natural language processing tasks such as text generation, language translation, and sentiment analysis. They are often used in scenarios where computational resources are limited or where rapid development and deployment of language models are required.\\n\\nSome popular small language models include BERT (Bidirectional Encoder Representations from Transformers) and GPT-2. These models have been pre-trained on large text corpora and fine-tuned for specific tasks to achieve good performance.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 12, 'total_tokens': 164, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CFlbh1pFAX8EYdjoW2nEvA1Tgct2m', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fb977412-e8f7-40d7-80f3-e8397d6186f1-0', usage_metadata={'input_tokens': 12, 'output_tokens': 152, 'total_tokens': 164, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_response=llm.invoke(\"What is Small Language Models\")\n",
    "test_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d64fd89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002108688EEC0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002108688DFF0>, root_client=<openai.OpenAI object at 0x000002108B5A2440>, root_async_client=<openai.AsyncOpenAI object at 0x000002108688F430>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models.base import init_chat_model\n",
    "\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\")\n",
    "#llm=init_chat_model(\"groq:\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4c69b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI, or artificial intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems. This includes tasks such as learning, reasoning, problem-solving, understanding natural language, and perception. AI technologies can be used to automate tasks, make predictions, and improve decision-making in a wide range of industries and applications.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 10, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CFlbj7uKUYX6Gjr2AHrZOwYuYln3y', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6a785724-6a59-4642-894b-07fb56d406dd-0', usage_metadata={'input_tokens': 10, 'output_tokens': 65, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7b210",
   "metadata": {},
   "source": [
    "## RAG-Kette erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6002655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c5d21d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002108B5A2BF0>, search_kwargs={})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(\n",
    "    search_kwarg={\"k\":3} \n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8b71ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant for question-answering tasks.  \n",
    "Use the retrieved context below to answer the user’s question.  \n",
    "\n",
    "- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \n",
    "- Be concise (max. 3 sentences).  \n",
    "- Ground your answer in the context, don’t invent facts.  \n",
    "\n",
    "Context:\n",
    "{context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "32622905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002108688EEC0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002108688DFF0>, root_client=<openai.OpenAI object at 0x000002108B5A2440>, root_async_client=<openai.AsyncOpenAI object at 0x000002108688F430>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b263c5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002108B5A2BF0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002108688EEC0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002108688DFF0>, root_client=<openai.OpenAI object at 0x000002108B5A2440>, root_async_client=<openai.AsyncOpenAI object at 0x000002108688F430>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "rag_chain=create_retrieval_chain(retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "35d0daec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How can I apply RAG in building a chatbot?',\n",
       " 'context': [Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_architecture.txt'}, page_content='The RAG architecture typically consists of two parts: \\na retriever and a generator. The retriever finds relevant documents \\nfrom a knowledge base, while the generator (often a large language model) \\nuses those documents to produce responses.')],\n",
       " 'answer': \"You can apply RAG in building a chatbot by using the retriever to find relevant documents from a knowledge base and the generator - usually a large language model - to produce responses based on those documents. This improves the reliability of the chatbot's responses by grounding them in factual information.\"}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=rag_chain.invoke({\"input\":\"How can I apply RAG in building a chatbot?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "82bfb67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You can apply RAG in building a chatbot by using the retriever to find relevant documents from a knowledge base and the generator - usually a large language model - to produce responses based on those documents. This improves the reliability of the chatbot's responses by grounding them in factual information.\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1_ChromaDB (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
