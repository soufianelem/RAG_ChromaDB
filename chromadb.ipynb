{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3699d895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c315fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e516e2",
   "metadata": {},
   "source": [
    "## Beispieldaten erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d2e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files created in 'data' folder: ['rag_intro.txt', 'rag_architecture.txt', 'rag_usecases.txt', 'rag_challenges.txt', 'rag_future.txt', 'rag_vs_finetuningvs_promptengineering.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "sample_docs = {\n",
    "    \"rag_intro.txt\": \"\"\"Retrieval-Augmented Generation (RAG) is a technique \n",
    "that combines information retrieval with generative models. \n",
    "It allows language models to access external knowledge sources \n",
    "to provide more accurate and up-to-date answers.\"\"\",\n",
    "\n",
    "    \"rag_architecture.txt\": \"\"\"The RAG architecture typically consists of two parts: \n",
    "a retriever and a generator. The retriever finds relevant documents \n",
    "from a knowledge base, while the generator (often a large language model) \n",
    "uses those documents to produce responses.\"\"\",\n",
    "\n",
    "    \"rag_usecases.txt\": \"\"\"RAG can be applied in many areas such as \n",
    "question answering, chatbots, document summarization, and enterprise search. \n",
    "It improves reliability by grounding responses in factual documents.\"\"\",\n",
    "\n",
    "\"rag_challenges.txt\": \"\"\"Some challenges of RAG include retrieving irrelevant documents,\n",
    "managing large-scale knowledge bases, and ensuring that the generator\n",
    "uses the retrieved documents correctly without introducing hallucinations.\"\"\",\n",
    "\n",
    "    \"rag_future.txt\": \"\"\"The future of RAG research focuses on improving retriever models,\n",
    "combining structured and unstructured data, and optimizing efficiency\n",
    "for real-world enterprise applications.\"\"\",\n",
    "\n",
    "    \"rag_vs_finetuningvs_promptengineering.txt\": \"\"\"RAG, fine-tuning, and prompt engineering differ in how they improve \n",
    "language models.\n",
    "\n",
    "Fine-tuning changes the model’s weights by training it on additional \n",
    "domain-specific data. This makes the model more accurate for that domain, \n",
    "but it is costly and needs retraining when knowledge changes.\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) does not change the model itself. \n",
    "Instead, it connects the model to an external retriever that provides \n",
    "relevant documents. The model then generates answers based on both its \n",
    "own knowledge and the retrieved context. This keeps results more current \n",
    "and fact-based.\n",
    "\n",
    "Prompt engineering does not retrain the model or add external data. \n",
    "It only improves how we ask questions by designing effective prompts. \n",
    "This method is simple and fast but limited, since it does not add \n",
    "new knowledge.\n",
    "\n",
    "In short: fine-tuning embeds knowledge, RAG retrieves knowledge, \n",
    "and prompt engineering shapes the way knowledge is used.\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filename, content in sample_docs.items():\n",
    "    with open(os.path.join(\"data\", filename), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Text files created in 'data' folder:\", list(sample_docs.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02003b5",
   "metadata": {},
   "source": [
    "## Text in Chunks aufteilen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66faeccd",
   "metadata": {},
   "source": [
    "## Dokumente laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe0fe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Dokumente geladen\n",
      "\n",
      "Vorschau erstes Dokument:\n",
      "The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "u...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"data\",                     \n",
    "    glob=\"*.txt\",              \n",
    "    loader_cls=TextLoader,      \n",
    "    loader_kwargs={'encoding': 'utf-8'}  \n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"{len(documents)} Dokumente geladen\")\n",
    "print(\"\\nVorschau erstes Dokument:\")\n",
    "print(documents[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed490a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Chunks aus 6 Dokumenten erstellt\n",
      "\n",
      "Chunk-Beispiel:\n",
      "Inhalt: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, whi...\n",
      "Metadaten: {'source': 'data\\\\rag_architecture.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,     \n",
    "    chunk_overlap=50,   \n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"{len(chunks)} Chunks aus {len(documents)} Dokumenten erstellt\")\n",
    "\n",
    "print(\"\\nChunk-Beispiel:\")\n",
    "print(f\"Inhalt: {chunks[0].page_content[:150]}...\")\n",
    "print(f\"Metadaten: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0869b663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\rag_architecture.txt'}, page_content='The RAG architecture typically consists of two parts: \\na retriever and a generator. The retriever finds relevant documents \\nfrom a knowledge base, while the generator (often a large language model) \\nuses those documents to produce responses.'),\n",
       " Document(metadata={'source': 'data\\\\rag_challenges.txt'}, page_content='Some challenges of RAG include retrieving irrelevant documents,\\nmanaging large-scale knowledge bases, and ensuring that the generator\\nuses the retrieved documents correctly without introducing hallucinations.'),\n",
       " Document(metadata={'source': 'data\\\\rag_future.txt'}, page_content='The future of RAG research focuses on improving retriever models,\\ncombining structured and unstructured data, and optimizing efficiency\\nfor real-world enterprise applications.'),\n",
       " Document(metadata={'source': 'data\\\\rag_intro.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a technique \\nthat combines information retrieval with generative models. \\nIt allows language models to access external knowledge sources \\nto provide more accurate and up-to-date answers.'),\n",
       " Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG (Retrieval-Augmented Generation) does not change the model itself. \\nInstead, it connects the model to an external retriever that provides \\nrelevant documents. The model then generates answers based on both its \\nown knowledge and the retrieved context. This keeps results more current \\nand fact-based.'),\n",
       " Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='Prompt engineering does not retrain the model or add external data. \\nIt only improves how we ask questions by designing effective prompts. \\nThis method is simple and fast but limited, since it does not add \\nnew knowledge.\\n\\nIn short: fine-tuning embeds knowledge, RAG retrieves knowledge, \\nand prompt engineering shapes the way knowledge is used.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953e64a",
   "metadata": {},
   "source": [
    "## Embedding-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f502daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac263f",
   "metadata": {},
   "source": [
    "## ChromaDB Vektordatenbank erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb4d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Vektordatenbank erstellt mit 8 Vektoren\n",
      "Gespeichert in: ./chroma_db\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"rag_collection\",\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "\n",
    "print(vectorstore._collection.count())\n",
    "print(f\"Vektordatenbank erstellt mit {vectorstore._collection.count()} Vektoren\")\n",
    "print(f\"Gespeichert in: {persist_directory}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b770b1c",
   "metadata": {},
   "source": [
    "## Ähnlichkeitssuche testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc6cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}, page_content='RAG, fine-tuning, and prompt engineering differ in how they improve \\nlanguage models.\\n\\nFine-tuning changes the model’s weights by training it on additional \\ndomain-specific data. This makes the model more accurate for that domain, \\nbut it is costly and needs retraining when knowledge changes.'),\n",
       " Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       " Document(metadata={'source': 'data\\\\rag_architecture.txt'}, page_content='The RAG architecture typically consists of two parts: \\na retriever and a generator. The retriever finds relevant documents \\nfrom a knowledge base, while the generator (often a large language model) \\nuses those documents to produce responses.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the difference between RAG and fine-tuning?\"\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0671774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ergebnis 1:\n",
      "Inhalt: RAG, fine-tuning, and prompt engineering differ in how they improve \n",
      "language models.\n",
      "\n",
      "Fine-tuning changes the model’s weights by training it on additional \n",
      "domain-specific data. This makes the model more accurate for that domain, \n",
      "but it is costly and needs retraining when knowledge changes.\n",
      "Metadaten: {'source': 'data\\\\rag_vs_finetuningvs_promptengineering.txt'}\n",
      "\n",
      "Ergebnis 2:\n",
      "Inhalt: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadaten: {'source': 'data\\\\rag_usecases.txt'}\n",
      "\n",
      "Ergebnis 3:\n",
      "Inhalt: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "uses those documents to produce responses.\n",
      "Metadaten: {'source': 'data\\\\rag_architecture.txt'}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(results, 1):\n",
    "   print(f\"\\nErgebnis {i}:\")\n",
    "   print(f\"Inhalt: {res.page_content}\")\n",
    "   print(f\"Metadaten: {res.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c951683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ergebnis 1:\n",
      "Inhalt: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadaten: {'source': 'data\\\\rag_usecases.txt'}\n",
      "\n",
      "Ergebnis 2:\n",
      "Inhalt: Some challenges of RAG include retrieving irrelevant documents,\n",
      "managing large-scale knowledge bases, and ensuring that the generator\n",
      "uses the retrieved documents correctly without introducing hallucinations.\n",
      "Metadaten: {'source': 'data\\\\rag_challenges.txt'}\n",
      "\n",
      "Ergebnis 3:\n",
      "Inhalt: The future of RAG research focuses on improving retriever models,\n",
      "combining structured and unstructured data, and optimizing efficiency\n",
      "for real-world enterprise applications.\n",
      "Metadaten: {'source': 'data\\\\rag_future.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are some use cases of RAG?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"\\nErgebnis {i}:\")\n",
    "    print(f\"Inhalt: {res.page_content}\")\n",
    "    print(f\"Metadaten: {res.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51672581",
   "metadata": {},
   "source": [
    "## Ähnlichkeitssuche mit Bewertung\n",
    "\n",
    "ChromaDB verwendet standardmäßig L2-Distanz:\n",
    "- 0.0 = perfekte Übereinstimmung  \n",
    "- Näher zu 0 = relevanter\n",
    "- Höhere Werte = weniger relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "494c5d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ergebnis 1 (Bewertung=0.2337):\n",
      "Inhalt: The RAG architecture typically consists of two parts: \n",
      "a retriever and a generator. The retriever finds relevant documents \n",
      "from a knowledge base, while the generator (often a large language model) \n",
      "uses those documents to produce responses.\n",
      "Metadaten: {'source': 'data\\\\rag_architecture.txt'}\n",
      "\n",
      "Ergebnis 2 (Bewertung=0.3461):\n",
      "Inhalt: Some challenges of RAG include retrieving irrelevant documents,\n",
      "managing large-scale knowledge bases, and ensuring that the generator\n",
      "uses the retrieved documents correctly without introducing hallucinations.\n",
      "Metadaten: {'source': 'data\\\\rag_challenges.txt'}\n",
      "\n",
      "Ergebnis 3 (Bewertung=0.3538):\n",
      "Inhalt: RAG can be applied in many areas such as \n",
      "question answering, chatbots, document summarization, and enterprise search. \n",
      "It improves reliability by grounding responses in factual documents.\n",
      "Metadaten: {'source': 'data\\\\rag_usecases.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main components of the RAG architecture?\"\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nErgebnis {i} (Bewertung={score:.4f}):\")\n",
    "    print(f\"Inhalt: {doc.page_content}\")\n",
    "    print(f\"Metadaten: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf6abef",
   "metadata": {},
   "source": [
    "## Sprachmodell initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd0cbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "365005e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Small language models refer to natural language processing models that have a relatively small number of parameters compared to larger language models. These models are typically used for simpler tasks that do not require as much computational power or resources, such as text classification, sentiment analysis, or language generation on a small scale.\\n\\nSmall language models are often trained on fewer examples and may have limited capabilities compared to larger models like GPT-3 or BERT. However, they can still be effective for certain applications and are more accessible for researchers or developers with limited resources. Additionally, small language models can be fine-tuned on specific datasets to improve performance on specific tasks.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 127, 'prompt_tokens': 12, 'total_tokens': 139, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CFmFzJKmwNbNqmSI6IQk3iOFvZ0AF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5f76f626-e2e6-4c89-8cde-a1435f5a3044-0', usage_metadata={'input_tokens': 12, 'output_tokens': 127, 'total_tokens': 139, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_response=llm.invoke(\"What is Small Language Models\")\n",
    "test_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64fd89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000017F41361600>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017F413611B0>, root_client=<openai.OpenAI object at 0x0000017F44CE3D60>, root_async_client=<openai.AsyncOpenAI object at 0x0000017F41362A70>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models.base import init_chat_model\n",
    "\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\")\n",
    "#llm=init_chat_model(\"groq:\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4c69b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI, or artificial intelligence, is the simulation of human intelligence processes by machines, especially computer systems. This includes the ability to learn, reason, make decisions, and understand natural language. AI is used in a wide range of applications, including speech recognition, image recognition, autonomous vehicles, and more.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 60, 'prompt_tokens': 10, 'total_tokens': 70, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CFmG8P4vIY9F2Tf2AWWpflkjFaQra', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6adf4364-91c1-4bc1-9c0d-9897a01633d8-0', usage_metadata={'input_tokens': 10, 'output_tokens': 60, 'total_tokens': 70, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7b210",
   "metadata": {},
   "source": [
    "## RAG-Kette erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6002655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d21d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000017F41870D30>, search_kwargs={})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(\n",
    "    search_kwarg={\"k\":3} \n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8b71ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant for question-answering tasks.  \n",
    "Use the retrieved context below to answer the user’s question.  \n",
    "\n",
    "- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \n",
    "- Be concise (max. 3 sentences).  \n",
    "- Ground your answer in the context, don’t invent facts.  \n",
    "\n",
    "Context:\n",
    "{context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32622905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000017F41361600>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017F413611B0>, root_client=<openai.OpenAI object at 0x0000017F44CE3D60>, root_async_client=<openai.AsyncOpenAI object at 0x0000017F41362A70>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b263c5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000017F41870D30>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are a helpful assistant for question-answering tasks.  \\nUse the retrieved context below to answer the user’s question.  \\n\\n- If the answer is not in the context, say: \"I don’t know based on the provided documents.\"  \\n- Be concise (max. 3 sentences).  \\n- Ground your answer in the context, don’t invent facts.  \\n\\nContext:\\n{context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000017F41361600>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017F413611B0>, root_client=<openai.OpenAI object at 0x0000017F44CE3D60>, root_async_client=<openai.AsyncOpenAI object at 0x0000017F41362A70>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "rag_chain=create_retrieval_chain(retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35d0daec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How can I apply RAG in building a chatbot?',\n",
       " 'context': [Document(metadata={'source': 'data\\\\rag_usecases.txt'}, page_content='RAG can be applied in many areas such as \\nquestion answering, chatbots, document summarization, and enterprise search. \\nIt improves reliability by grounding responses in factual documents.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_architecture.txt'}, page_content='The RAG architecture typically consists of two parts: \\na retriever and a generator. The retriever finds relevant documents \\nfrom a knowledge base, while the generator (often a large language model) \\nuses those documents to produce responses.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_challenges.txt'}, page_content='Some challenges of RAG include retrieving irrelevant documents,\\nmanaging large-scale knowledge bases, and ensuring that the generator\\nuses the retrieved documents correctly without introducing hallucinations.'),\n",
       "  Document(metadata={'source': 'data\\\\rag_intro.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a technique \\nthat combines information retrieval with generative models. \\nIt allows language models to access external knowledge sources \\nto provide more accurate and up-to-date answers.')],\n",
       " 'answer': 'You can apply RAG in building a chatbot by using the retriever to find relevant documents from a knowledge base, and then the generator (like a large language model) can use those documents to produce responses. This combination allows the chatbot to access external knowledge sources to provide more accurate and up-to-date answers, improving the reliability of the responses.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=rag_chain.invoke({\"input\":\"How can I apply RAG in building a chatbot?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82bfb67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can apply RAG in building a chatbot by using the retriever to find relevant documents from a knowledge base, and then the generator (like a large language model) can use those documents to produce responses. This combination allows the chatbot to access external knowledge sources to provide more accurate and up-to-date answers, improving the reliability of the responses.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1_ChromaDB (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
